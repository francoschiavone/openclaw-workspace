/**
 * Together.ai Inference API client.
 *
 * Cloud-based AI inference using Together.ai's API.
 * Supports Qwen3-14B, Qwen 2.5-72B, and other models.
 *
 * This is the PRIMARY inference provider ($50/month budget).
 * Falls back to local llama.cpp (qwen-inference.ts) if unavailable.
 *
 * @see https://docs.together.ai/reference/chat-completions
 */
import type { QwenDomain, QwenSuggestOptions } from './qwen-inference.js';
/**
 * Available models on Together.ai (sorted by quality).
 *
 * @see https://docs.together.ai/docs/serverless-models
 */
export declare const TOGETHER_MODELS: {
    readonly 'qwen3-14b': "Qwen/Qwen3-14B";
    readonly 'qwen3-14b-instruct': "Qwen/Qwen3-14B-Instruct";
    readonly 'qwen2.5-72b': "Qwen/Qwen2.5-72B-Instruct";
    readonly 'qwen2.5-32b': "Qwen/Qwen2.5-32B-Instruct";
    readonly 'qwen2.5-7b': "Qwen/Qwen2.5-7B-Instruct";
    readonly default: "Qwen/Qwen3-14B-Instruct";
};
export type TogetherModelKey = keyof typeof TOGETHER_MODELS;
/**
 * Together.ai specific options extending base options.
 */
export interface TogetherSuggestOptions extends QwenSuggestOptions {
    /** Model to use (default: qwen3-14b-instruct) */
    model?: TogetherModelKey;
}
/**
 * Custom error for Together.ai inference failures.
 */
export declare class TogetherInferenceError extends Error {
    readonly code: 'TIMEOUT' | 'RATE_LIMIT' | 'INVALID_RESPONSE' | 'SERVER_ERROR' | 'NOT_CONFIGURED' | 'INSUFFICIENT_CREDITS';
    readonly statusCode?: number | undefined;
    constructor(message: string, code: 'TIMEOUT' | 'RATE_LIMIT' | 'INVALID_RESPONSE' | 'SERVER_ERROR' | 'NOT_CONFIGURED' | 'INSUFFICIENT_CREDITS', statusCode?: number | undefined);
}
/**
 * Together.ai Inference API client with retry logic and caching.
 */
export declare class TogetherInferenceClient {
    private readonly apiKey;
    private readonly timeoutMs;
    private readonly maxRetries;
    private readonly defaultModel;
    private readonly cache;
    private readonly circuitBreaker;
    constructor(apiKey: string, options?: {
        timeoutMs?: number;
        maxRetries?: number;
        cacheTtl?: number;
        defaultModel?: TogetherModelKey;
    });
    /**
     * Generate domain suggestions using Together.ai.
     *
     * Returns suggestions or null if Together.ai is unavailable.
     * Graceful degradation - caller should fall back to local inference.
     */
    suggest(options: TogetherSuggestOptions): Promise<QwenDomain[] | null>;
    /**
     * Build chat messages for Together.ai API.
     */
    private _buildMessages;
    /**
     * Calculate max_tokens based on number of suggestions and style.
     *
     * Style-aware token allocation reduces API costs by 20-30%.
     */
    private _calculateMaxTokens;
    /**
     * Parse domain names from model-generated text.
     */
    private _parseDomainsFromText;
    /**
     * Make HTTP request with timeout and error handling.
     */
    private _makeRequest;
    /**
     * Make request with exponential backoff retry.
     */
    private _makeRequestWithRetry;
}
/**
 * Get Together.ai client instance (singleton).
 *
 * Returns null if Together.ai is not configured.
 * Caller should fall back to local qwen-inference.ts if null.
 */
export declare function getTogetherClient(): TogetherInferenceClient | null;
/**
 * Check if Together.ai is configured and available.
 */
export declare function isTogetherConfigured(): boolean;
//# sourceMappingURL=together-inference.d.ts.map